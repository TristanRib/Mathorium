{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Agent and Basic Target\n",
   "id": "f032ea74ec188ecc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "\n",
    "# Hyperparamètres\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.3\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "iterations = 5000\n",
    "\n",
    "# Taille de la grille\n",
    "GRID_SIZE = 20\n",
    "\n",
    "# Départ et but\n",
    "start = (0, 0, 0)\n",
    "goal = (GRID_SIZE - 1, GRID_SIZE - 1, GRID_SIZE - 1)\n",
    "\n",
    "# Définition des actions : (dx, dy, dz)\n",
    "actions = [\n",
    "    (1, 0, 0), (-1, 0, 0),  # droite / gauche\n",
    "    (0, 1, 0), (0, -1, 0),  # avant / arrière\n",
    "    (0, 0, 1), (0, 0, -1)   # haut / bas\n",
    "]\n",
    "\n",
    "# Initialisation Q-table\n",
    "Q = {}\n",
    "\n",
    "def get_q(state):\n",
    "    if state not in Q:\n",
    "        Q[state] = np.zeros(len(actions))\n",
    "    return Q[state]\n",
    "\n",
    "def is_valid(pos):\n",
    "    return all(0 <= p < GRID_SIZE for p in pos)\n",
    "\n",
    "def step(pos, action_idx):\n",
    "    dx, dy, dz = actions[action_idx]\n",
    "    new_pos = (pos[0] + dx, pos[1] + dy, pos[2] + dz)\n",
    "    if not is_valid(new_pos):\n",
    "        return pos, -1  # pénalité pour tentative hors limites\n",
    "    if new_pos == goal:\n",
    "        return new_pos, 10\n",
    "    return new_pos, -0.1\n",
    "\n",
    "# Suivi du meilleur chemin\n",
    "best_path = []\n",
    "shortest_length = float('inf')\n",
    "\n",
    "for episode in range(iterations):\n",
    "    state = start\n",
    "    path = [state]\n",
    "    total_reward = 0\n",
    "\n",
    "    while state != goal:\n",
    "        if random.random() < epsilon:\n",
    "            action_idx = random.randint(0, len(actions) - 1)\n",
    "        else:\n",
    "            action_idx = np.argmax(get_q(state))\n",
    "\n",
    "        next_state, reward = step(state, action_idx)\n",
    "        total_reward += reward\n",
    "\n",
    "        old_value = get_q(state)[action_idx]\n",
    "        next_max = np.max(get_q(next_state))\n",
    "        get_q(state)[action_idx] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "\n",
    "        state = next_state\n",
    "        path.append(state)\n",
    "\n",
    "        # Sécurité pour éviter des boucles infinies\n",
    "        if len(path) > GRID_SIZE**3:\n",
    "            break\n",
    "\n",
    "    # Sauvegarde du meilleur chemin\n",
    "    if state == goal and len(path) < shortest_length:\n",
    "        best_path = path\n",
    "        shortest_length = len(path)\n",
    "\n",
    "    # Décroissance de l'exploration\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "# --- Affichage avec Plotly ---\n",
    "x, y, z = zip(*best_path)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Tracé du chemin\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=x, y=y, z=z,\n",
    "    mode='lines+markers',\n",
    "    line=dict(color='blue', width=5),\n",
    "    marker=dict(size=4),\n",
    "    name='Best path'\n",
    "))\n",
    "\n",
    "# Départ (vert)\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=[start[0]], y=[start[1]], z=[start[2]],\n",
    "    mode='markers',\n",
    "    marker=dict(size=8, color='green'),\n",
    "    name='Start'\n",
    "))\n",
    "\n",
    "# But (rouge)\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=[goal[0]], y=[goal[1]], z=[goal[2]],\n",
    "    mode='markers',\n",
    "    marker=dict(size=8, color='red'),\n",
    "    name='Goal'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Best Path in 3D Grid (Q-Learning)',\n",
    "    scene=dict(\n",
    "        xaxis=dict(nticks=5, range=[0, GRID_SIZE]),\n",
    "        yaxis=dict(nticks=5, range=[0, GRID_SIZE]),\n",
    "        zaxis=dict(nticks=5, range=[0, GRID_SIZE]),\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=30)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Graph",
   "id": "72a325d5b9ca2449"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def display_graph(saved_agent_path, saved_target_path):\n",
    "    x_agent, y_agent, z_agent = zip(*saved_agent_path)\n",
    "    x_target, y_target, z_target = zip(*saved_target_path)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Trajectoire de l'agent\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=x_agent, y=y_agent, z=z_agent,\n",
    "        mode='lines+markers',\n",
    "        name='Agent',\n",
    "        line=dict(color='blue', width=4),\n",
    "        marker=dict(size=3)\n",
    "    ))\n",
    "\n",
    "    # Trajectoire de la cible\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=x_target, y=y_target, z=z_target,\n",
    "        mode='lines+markers',\n",
    "        name='Cible (nourriture)',\n",
    "        line=dict(color='orange', width=2, dash='dot'),\n",
    "        marker=dict(size=3)\n",
    "    ))\n",
    "\n",
    "    # Départ et arrivée de l'agent\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[x_agent[0]], y=[y_agent[0]], z=[z_agent[0]],\n",
    "        mode='markers',\n",
    "        name='Départ Agent',\n",
    "        marker=dict(color='green', size=6, symbol='circle')\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[x_agent[-1]], y=[y_agent[-1]], z=[z_agent[-1]],\n",
    "        mode='markers',\n",
    "        name='Arrivée Agent',\n",
    "        marker=dict(color='red', size=6, symbol='circle')\n",
    "    ))\n",
    "\n",
    "    # Départ et arrivée de la cible\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[x_target[0]], y=[y_target[0]], z=[z_target[0]],\n",
    "        mode='markers',\n",
    "        name='Départ Cible',\n",
    "        marker=dict(color='green', size=6, symbol='diamond')\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[x_target[-1]], y=[y_target[-1]], z=[z_target[-1]],\n",
    "        mode='markers',\n",
    "        name='Arrivée Cible',\n",
    "        marker=dict(color='red', size=6, symbol='diamond')\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        title='Trajectoire Agent et Cible dans l\\'environnement 3D',\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z',\n",
    "        ),\n",
    "        legend=dict(x=0, y=1)\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ],
   "id": "6ec75a25b7f476af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def display_graph_2d(saved_agent_path, saved_target_path, grid_size):\n",
    "    x_agent, y_agent = zip(*saved_agent_path)\n",
    "    x_target, y_target = zip(*saved_target_path)\n",
    "\n",
    "    # Déterminer les bornes min et max de la grille\n",
    "    all_x = x_agent + x_target\n",
    "    all_y = y_agent + y_target\n",
    "    min_x = grid_size * (min(all_x) // grid_size)\n",
    "    max_x = grid_size * ((max(all_x) // grid_size) + 1)\n",
    "    min_y = grid_size * (min(all_y) // grid_size)\n",
    "    max_y = grid_size * ((max(all_y) // grid_size) + 1)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Trajectoire de l'agent\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_agent, y=y_agent,\n",
    "        mode='lines+markers',\n",
    "        name='Agent',\n",
    "        line=dict(color='blue', width=3),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "    # Trajectoire de la cible\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_target, y=y_target,\n",
    "        mode='lines+markers',\n",
    "        name='Cible (nourriture)',\n",
    "        line=dict(color='orange', width=2, dash='dot'),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "    # Départ et arrivée de l'agent\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[x_agent[0]], y=[y_agent[0]],\n",
    "        mode='markers',\n",
    "        name='Départ Agent',\n",
    "        marker=dict(color='green', size=10, symbol='circle')\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[x_agent[-1]], y=[y_agent[-1]],\n",
    "        mode='markers',\n",
    "        name='Arrivée Agent',\n",
    "        marker=dict(color='red', size=10, symbol='circle')\n",
    "    ))\n",
    "\n",
    "    # Départ et arrivée de la cible\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[x_target[0]], y=[y_target[0]],\n",
    "        mode='markers',\n",
    "        name='Départ Cible',\n",
    "        marker=dict(color='green', size=10, symbol='diamond')\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[x_target[-1]], y=[y_target[-1]],\n",
    "        mode='markers',\n",
    "        name='Arrivée Cible',\n",
    "        marker=dict(color='red', size=10, symbol='diamond')\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=1600,\n",
    "        template='plotly_dark',\n",
    "        title='Trajectoire Agent et Cible dans l\\'environnement 2D',\n",
    "        xaxis=dict(\n",
    "            title='X',\n",
    "            tickmode='linear',\n",
    "            dtick=grid_size,\n",
    "            range=[min_x, max_x],\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor='lightgrey',\n",
    "            scaleanchor='y'  # Fixe le rapport 1:1 avec l'axe Y\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Y',\n",
    "            tickmode='linear',\n",
    "            dtick=grid_size,\n",
    "            range=[min_y, max_y],\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor='lightgrey'\n",
    "        ),\n",
    "        legend=dict(x=0.02, y=0.98),\n",
    "        template='plotly_white'\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ],
   "id": "9051010f546a75db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def display_animation(saved_agent_path, saved_target_path):\n",
    "    x_agent, y_agent, z_agent = zip(*saved_agent_path)\n",
    "    x_target, y_target, z_target = zip(*saved_target_path)\n",
    "\n",
    "    frames = []\n",
    "    for i in range(len(x_agent)):\n",
    "        frames.append(go.Frame(\n",
    "            data=[\n",
    "                go.Scatter3d(x=x_agent[:i+1], y=y_agent[:i+1], z=z_agent[:i+1],\n",
    "                             mode='lines+markers', name='Agent',\n",
    "                             line=dict(color='blue', width=4),\n",
    "                             marker=dict(size=3)),\n",
    "                go.Scatter3d(x=x_target[:i+1], y=y_target[:i+1], z=z_target[:i+1],\n",
    "                             mode='lines+markers', name='Cible (nourriture)',\n",
    "                             line=dict(color='orange', width=2, dash='dot'),\n",
    "                             marker=dict(size=3))\n",
    "            ],\n",
    "            name=str(i)\n",
    "        ))\n",
    "\n",
    "    # Figure de base avec les premières positions uniquement\n",
    "    fig = go.Figure(\n",
    "        data=[\n",
    "            go.Scatter3d(x=[x_agent[0]], y=[y_agent[0]], z=[z_agent[0]],\n",
    "                         mode='lines+markers', name='Agent',\n",
    "                         line=dict(color='blue', width=4),\n",
    "                         marker=dict(size=3)),\n",
    "            go.Scatter3d(x=[x_target[0]], y=[y_target[0]], z=[z_target[0]],\n",
    "                         mode='lines+markers', name='Cible (nourriture)',\n",
    "                         line=dict(color='orange', width=2, dash='dot'),\n",
    "                         marker=dict(size=3)),\n",
    "            # Départ / Arrivée Agent\n",
    "            go.Scatter3d(x=[x_agent[0]], y=[y_agent[0]], z=[z_agent[0]],\n",
    "                         mode='markers', name='Départ Agent',\n",
    "                         marker=dict(color='green', size=6, symbol='circle')),\n",
    "            go.Scatter3d(x=[x_agent[-1]], y=[y_agent[-1]], z=[z_agent[-1]],\n",
    "                         mode='markers', name='Arrivée Agent',\n",
    "                         marker=dict(color='red', size=6, symbol='circle')),\n",
    "            # Départ / Arrivée Cible\n",
    "            go.Scatter3d(x=[x_target[0]], y=[y_target[0]], z=[z_target[0]],\n",
    "                         mode='markers', name='Départ Cible',\n",
    "                         marker=dict(color='green', size=6, symbol='diamond')),\n",
    "            go.Scatter3d(x=[x_target[-1]], y=[y_target[-1]], z=[z_target[-1]],\n",
    "                         mode='markers', name='Arrivée Cible',\n",
    "                         marker=dict(color='red', size=6, symbol='diamond'))\n",
    "        ],\n",
    "        layout=go.Layout(\n",
    "            height= 900,\n",
    "            title='Trajectoire Agent et Cible dans l\\'environnement 3D',\n",
    "            scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'),\n",
    "            updatemenus=[dict(\n",
    "                type='buttons',\n",
    "                showactive=False,\n",
    "                y=1.15,\n",
    "                x=1.05,\n",
    "                xanchor='right',\n",
    "                yanchor='top',\n",
    "                buttons=[dict(label='Play',\n",
    "                              method='animate',\n",
    "                              args=[None, {\"frame\": {\"duration\": 100, \"redraw\": True},\n",
    "                                           \"fromcurrent\": True, \"transition\": {\"duration\": 0}}]),\n",
    "                         dict(label='Pause',\n",
    "                              method='animate',\n",
    "                              args=[[None], {\"frame\": {\"duration\": 0, \"redraw\": False},\n",
    "                                             \"mode\": \"immediate\",\n",
    "                                             \"transition\": {\"duration\": 0}}])]\n",
    "            )],\n",
    "            sliders=[dict(\n",
    "                steps=[dict(method='animate',\n",
    "                            args=[[str(k)], {\"frame\": {\"duration\": 0, \"redraw\": True},\n",
    "                                             \"mode\": \"immediate\",\n",
    "                                             \"transition\": {\"duration\": 0}}],\n",
    "                            label=str(k)) for k in range(len(x_agent))],\n",
    "                transition=dict(duration=0),\n",
    "                x=0.1, y=0, currentvalue=dict(font=dict(size=12), prefix=\"Étape : \", visible=True, xanchor='right'),\n",
    "                len=0.9\n",
    "            )]\n",
    "        ),\n",
    "        frames=frames\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ],
   "id": "58a973f6d41b4692",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def display_animation_2d(saved_agent_path, saved_target_path):\n",
    "    x_agent, y_agent = zip(*saved_agent_path)\n",
    "    x_target, y_target = zip(*saved_target_path)\n",
    "\n",
    "    frames = []\n",
    "    for i in range(len(x_agent)):\n",
    "        frames.append(go.Frame(\n",
    "            data=[\n",
    "                go.Scatter(x=x_agent[:i+1], y=y_agent[:i+1],\n",
    "                           mode='lines+markers', name='Agent',\n",
    "                           line=dict(color='blue', width=4),\n",
    "                           marker=dict(size=6)),\n",
    "                go.Scatter(x=x_target[:i+1], y=y_target[:i+1],\n",
    "                           mode='lines+markers', name='Cible (nourriture)',\n",
    "                           line=dict(color='orange', width=2, dash='dot'),\n",
    "                           marker=dict(size=6))\n",
    "            ],\n",
    "            name=str(i)\n",
    "        ))\n",
    "\n",
    "    # Figure de base avec les premières positions uniquement\n",
    "    fig = go.Figure(\n",
    "        data=[\n",
    "            go.Scatter(x=[x_agent[0]], y=[y_agent[0]],\n",
    "                       mode='lines+markers', name='Agent',\n",
    "                       line=dict(color='blue', width=4),\n",
    "                       marker=dict(size=6)),\n",
    "            go.Scatter(x=[x_target[0]], y=[y_target[0]],\n",
    "                       mode='lines+markers', name='Cible (nourriture)',\n",
    "                       line=dict(color='orange', width=2, dash='dot'),\n",
    "                       marker=dict(size=6)),\n",
    "            # Départ / Arrivée Agent\n",
    "            go.Scatter(x=[x_agent[0]], y=[y_agent[0]],\n",
    "                       mode='markers', name='Départ Agent',\n",
    "                       marker=dict(color='green', size=10, symbol='circle')),\n",
    "            go.Scatter(x=[x_agent[-1]], y=[y_agent[-1]],\n",
    "                       mode='markers', name='Arrivée Agent',\n",
    "                       marker=dict(color='red', size=10, symbol='circle')),\n",
    "            # Départ / Arrivée Cible\n",
    "            go.Scatter(x=[x_target[0]], y=[y_target[0]],\n",
    "                       mode='markers', name='Départ Cible',\n",
    "                       marker=dict(color='green', size=10, symbol='diamond')),\n",
    "            go.Scatter(x=[x_target[-1]], y=[y_target[-1]],\n",
    "                       mode='markers', name='Arrivée Cible',\n",
    "                       marker=dict(color='red', size=10, symbol='diamond'))\n",
    "        ],\n",
    "        layout=go.Layout(\n",
    "            height=1200,\n",
    "            title='Trajectoire Agent et Cible dans l\\'environnement 2D',\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            xaxis=dict(\n",
    "                tickmode='linear',\n",
    "                showgrid=True,  # Afficher la grille\n",
    "                gridwidth=1,  # Largeur des lignes de grille\n",
    "                gridcolor='LightGrey',  # Couleur des lignes de grille\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                tickmode='linear',\n",
    "                showgrid=True,  # Afficher la grille\n",
    "                gridwidth=1,  # Largeur des lignes de grille\n",
    "                gridcolor='LightGrey',  # Couleur des lignes de grille\n",
    "            ),\n",
    "            updatemenus=[dict(\n",
    "                type='buttons',\n",
    "                showactive=False,\n",
    "                y=1.15,\n",
    "                x=1.05,\n",
    "                xanchor='right',\n",
    "                yanchor='top',\n",
    "                buttons=[dict(label='Play',\n",
    "                              method='animate',\n",
    "                              args=[None, {\"frame\": {\"duration\": 100, \"redraw\": True},\n",
    "                                           \"fromcurrent\": True, \"transition\": {\"duration\": 0}}]),\n",
    "                         dict(label='Pause',\n",
    "                              method='animate',\n",
    "                              args=[[None], {\"frame\": {\"duration\": 0, \"redraw\": False},\n",
    "                                             \"mode\": \"immediate\",\n",
    "                                             \"transition\": {\"duration\": 0}}])],\n",
    "            )],\n",
    "            sliders=[dict(\n",
    "                steps=[dict(method='animate',\n",
    "                            args=[[str(k)], {\"frame\": {\"duration\": 0, \"redraw\": True},\n",
    "                                             \"mode\": \"immediate\",\n",
    "                                             \"transition\": {\"duration\": 0}}],\n",
    "                            label=str(k)) for k in range(len(x_agent))],\n",
    "                transition=dict(duration=0),\n",
    "                x=0.1, y=0, currentvalue=dict(font=dict(size=12), prefix=\"Étape : \", visible=True, xanchor='right'),\n",
    "                len=0.9\n",
    "            )]\n",
    "        ),\n",
    "        frames=frames\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ],
   "id": "3f9779e5f7a0bfe3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Agent and Moving Target",
   "id": "918daeb8ff6e9553"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "\n",
    "# Hyperparamètres\n",
    "alpha = 0.2\n",
    "gamma = 0.9\n",
    "epsilon = 0.3\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "iterations = 3000\n",
    "\n",
    "# Taille de la grille\n",
    "GRID_SIZE = 20\n",
    "MAX_DISTANCE = 3 * (GRID_SIZE - 1)\n",
    "\n",
    "# Départ et but\n",
    "start_agent = (GRID_SIZE / 2 - 1, 0, GRID_SIZE / 2 - 1)\n",
    "start_target = (GRID_SIZE / 2 - 1 , GRID_SIZE - 1, GRID_SIZE / 2 - 1)\n",
    "\n",
    "# Définition des actions : (dx, dy, dz)\n",
    "actions = [\n",
    "    (1, 0, 0), (-1, 0, 0),  # droite / gauche\n",
    "    (0, 1, 0), (0, -1, 0),  # avant / arrière\n",
    "    (0, 0, 1), (0, 0, -1)   # haut / bas\n",
    "]\n",
    "\n",
    "# Initialisation Q-table\n",
    "Q = {}\n",
    "\n",
    "def distance_to_goal(pos, goal):\n",
    "    dx = pos[0] - goal[0]\n",
    "    dy = pos[1] - goal[1]\n",
    "    dz = pos[2] - goal[2]\n",
    "    return abs(dx) + abs(dy) + abs(dz)\n",
    "\n",
    "def get_q(state, goal):\n",
    "    key = tuple(state + goal)\n",
    "    if key not in Q:\n",
    "        Q[key] = np.zeros(len(actions))\n",
    "    return Q[key]\n",
    "\n",
    "def is_valid(pos):\n",
    "    return all(0 <= p < GRID_SIZE for p in pos)\n",
    "\n",
    "def agent_step(pos, action_idx):\n",
    "    dx, dy, dz = actions[action_idx]\n",
    "    new_pos = (pos[0] + dx, pos[1] + dy, pos[2] + dz)\n",
    "\n",
    "    if not is_valid(new_pos):\n",
    "        return pos, -1\n",
    "\n",
    "    if new_pos == goal:\n",
    "        return new_pos, 10\n",
    "\n",
    "    new_reward = -0.1 + max(0, (MAX_DISTANCE - distance_to_goal(new_pos, goal)) / MAX_DISTANCE) * 0.5\n",
    "\n",
    "    return new_pos, new_reward\n",
    "\n",
    "def target_step(pos):\n",
    "    dx, dy, dz = actions[random.randint(0, len(actions) - 1)]\n",
    "    new_pos = (pos[0] + dx, pos[1] + dy, pos[2] + dz)\n",
    "    if not is_valid(new_pos):\n",
    "        return pos\n",
    "    return new_pos\n",
    "\n",
    "# Suivi du meilleur chemin\n",
    "saved_agent_path = []\n",
    "saved_target_path = []\n",
    "shortest_length = float('inf')\n",
    "\n",
    "for episode in range(iterations):\n",
    "    state = start_agent\n",
    "    goal = start_target\n",
    "    path = [state]\n",
    "    target_path = [goal]\n",
    "    total_reward = 0\n",
    "\n",
    "    while state != goal:\n",
    "        if random.random() < epsilon:\n",
    "            action_idx = random.randint(0, len(actions) - 1)\n",
    "        else:\n",
    "            action_idx = np.argmax(get_q(state, goal))\n",
    "\n",
    "        next_state, reward = agent_step(state, action_idx)\n",
    "        next_goal = target_step(goal)\n",
    "        total_reward += reward\n",
    "\n",
    "        old_value = get_q(state, goal)[action_idx]\n",
    "        next_max = np.max(get_q(next_state, next_goal))\n",
    "        get_q(state, goal)[action_idx] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "\n",
    "        state = next_state\n",
    "        goal = next_goal\n",
    "        path.append(state)\n",
    "        target_path.append(goal)\n",
    "\n",
    "        # Sécurité pour éviter des boucles infinies\n",
    "        if len(path) > GRID_SIZE**3:\n",
    "            break\n",
    "\n",
    "    # Sauvegarde du meilleur chemin\n",
    "    if state == goal and len(path) < shortest_length:\n",
    "        saved_agent_path = path\n",
    "        saved_target_path = target_path\n",
    "        shortest_length = len(path)\n",
    "\n",
    "    # Décroissance de l'exploration\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "\n",
    "display_graph(saved_agent_path, saved_target_path)"
   ],
   "id": "57d2b9cb7681c68a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display_animation(saved_agent_path, saved_target_path)",
   "id": "8b431949840da7c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distance based state",
   "id": "675cac55c8346f5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Hyperparamètres\n",
    "alpha = 0.2\n",
    "gamma = 0.9\n",
    "epsilon = 0.3\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "iterations = 3000\n",
    "\n",
    "# Taille de la grille\n",
    "GRID_SIZE = 20\n",
    "MAX_DISTANCE = 3 * (GRID_SIZE - 1)\n",
    "\n",
    "# Départ et but\n",
    "start_agent = (0, 0, 0)\n",
    "start_target = (GRID_SIZE - 1 , GRID_SIZE - 1, GRID_SIZE - 1)\n",
    "\n",
    "# Définition des actions : (dx, dy, dz)\n",
    "actions = [\n",
    "    (1, 0, 0), (-1, 0, 0),  # droite / gauche\n",
    "    (0, 1, 0), (0, -1, 0),  # avant / arrière\n",
    "    (0, 0, 1), (0, 0, -1)   # haut / bas\n",
    "]\n",
    "\n",
    "# Initialisation Q-table\n",
    "Q = {}\n",
    "\n",
    "def distances_to_goal(pos, goal):\n",
    "    dx = pos[0] - goal[0]\n",
    "    dy = pos[1] - goal[1]\n",
    "    dz = pos[2] - goal[2]\n",
    "    return dx, dy, dz\n",
    "\n",
    "def distance_to_goal(pos, goal):\n",
    "    dx, dy, dz = distances_to_goal(pos, goal)\n",
    "    return abs(dx) + abs(dy) + abs(dz)\n",
    "\n",
    "def get_q(state):\n",
    "    if state not in Q:\n",
    "        Q[state] = np.zeros(len(actions))\n",
    "    return Q[state]\n",
    "\n",
    "def is_valid(pos):\n",
    "    return all(0 <= p < GRID_SIZE for p in pos)\n",
    "\n",
    "def agent_step(pos, action_idx):\n",
    "    dx, dy, dz = actions[action_idx]\n",
    "    new_pos = (pos[0] + dx, pos[1] + dy, pos[2] + dz)\n",
    "\n",
    "    if not is_valid(new_pos):\n",
    "        return pos, distance_to_goal(pos, goal), -1\n",
    "\n",
    "    if new_pos == goal:\n",
    "        return new_pos, distance_to_goal(new_pos, goal), 10\n",
    "\n",
    "    new_reward = -0.1 + max(0, (MAX_DISTANCE - distance_to_goal(new_pos, goal)) / MAX_DISTANCE) * 0.5\n",
    "\n",
    "    return new_pos, distance_to_goal(new_pos, goal), new_reward\n",
    "\n",
    "def target_step(pos):\n",
    "    dx, dy, dz = actions[random.randint(0, len(actions) - 1)]\n",
    "    new_pos = (pos[0] + dx, pos[1] + dy, pos[2] + dz)\n",
    "    if not is_valid(new_pos):\n",
    "        return pos\n",
    "    return new_pos\n",
    "\n",
    "# Suivi du meilleur chemin\n",
    "saved_agent_path = []\n",
    "saved_target_path = []\n",
    "shortest_length = float('inf')\n",
    "\n",
    "for episode in range(iterations):\n",
    "    agent = start_agent\n",
    "    goal = start_target\n",
    "\n",
    "    state = distances_to_goal(agent, goal)\n",
    "    total_reward = 0\n",
    "\n",
    "    path = [agent]\n",
    "    target_path = [goal]\n",
    "\n",
    "    while agent != goal:\n",
    "        if random.random() < epsilon:\n",
    "            action_idx = random.randint(0, len(actions) - 1)\n",
    "        else:\n",
    "            action_idx = np.argmax(get_q(state))\n",
    "\n",
    "        next_position, next_state, reward = agent_step(agent, action_idx)\n",
    "        next_goal = target_step(goal)\n",
    "        total_reward += reward\n",
    "\n",
    "        old_value = get_q(state)[action_idx]\n",
    "        next_max = np.max(get_q(next_state))\n",
    "        get_q(state)[action_idx] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "\n",
    "        agent = next_position\n",
    "        goal = next_goal\n",
    "        state = next_state\n",
    "        path.append(agent)\n",
    "        target_path.append(goal)\n",
    "\n",
    "        # Sécurité pour éviter des boucles infinies\n",
    "        if len(path) > GRID_SIZE**3:\n",
    "            break\n",
    "\n",
    "    # Sauvegarde du meilleur chemin\n",
    "    if agent == goal and len(path) < shortest_length:\n",
    "        saved_agent_path = path\n",
    "        saved_target_path = target_path\n",
    "        shortest_length = len(path)\n",
    "\n",
    "    # Décroissance de l'exploration\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "## Plot\n",
    "\n",
    "# Déballage des positions\n",
    "display_graph(saved_agent_path, saved_target_path)\n"
   ],
   "id": "268a86aab08aef93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display_animation(saved_agent_path, saved_target_path)",
   "id": "a0c5a516ed2d94f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Penalize agent if does not reduce distance",
   "id": "a3ffce71ecb68f8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Hyperparamètres\n",
    "alpha = 0.2\n",
    "gamma = 0.9\n",
    "epsilon = 0.3\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "iterations = 3000\n",
    "\n",
    "# Taille de la grille\n",
    "GRID_SIZE = 20\n",
    "MAX_DISTANCE = 3 * (GRID_SIZE - 1)\n",
    "\n",
    "# Départ et but\n",
    "start_agent = (0, 0, 0)\n",
    "start_target = (GRID_SIZE - 1 , GRID_SIZE - 1, GRID_SIZE - 1)\n",
    "\n",
    "# Définition des actions : (dx, dy, dz)\n",
    "actions = [\n",
    "    (1, 0, 0), (-1, 0, 0),  # droite / gauche\n",
    "    (0, 1, 0), (0, -1, 0),  # avant / arrière\n",
    "    (0, 0, 1), (0, 0, -1)   # haut / bas\n",
    "]\n",
    "\n",
    "# Initialisation Q-table\n",
    "Q = {}\n",
    "\n",
    "def distances_to_goal(pos, goal):\n",
    "    dx = pos[0] - goal[0]\n",
    "    dy = pos[1] - goal[1]\n",
    "    dz = pos[2] - goal[2]\n",
    "    return dx, dy, dz\n",
    "\n",
    "def distance_to_goal(pos, goal):\n",
    "    dx, dy, dz = distances_to_goal(pos, goal)\n",
    "    return abs(dx) + abs(dy) + abs(dz)\n",
    "\n",
    "def get_q(state):\n",
    "    if state not in Q:\n",
    "        Q[state] = np.zeros(len(actions))\n",
    "    return Q[state]\n",
    "\n",
    "def is_valid(pos):\n",
    "    return all(0 <= p < GRID_SIZE for p in pos)\n",
    "\n",
    "def agent_step(agent, action_idx, goal, path):\n",
    "    dx, dy, dz = actions[action_idx]\n",
    "    new_agent_pos = (agent[0] + dx, agent[1] + dy, agent[2] + dz)\n",
    "\n",
    "    if not is_valid(new_agent_pos):\n",
    "        return agent, distance_to_goal(agent, goal), -1\n",
    "\n",
    "    if new_agent_pos == goal:\n",
    "        return new_agent_pos, distance_to_goal(new_agent_pos, goal), 10\n",
    "\n",
    "    if new_agent_pos in path[-10:]:\n",
    "        recent_index = path[-10:].index(new_agent_pos)\n",
    "        penalty = -0.2 * (recent_index + 1)\n",
    "        return new_agent_pos, distance_to_goal(new_agent_pos, goal), penalty\n",
    "\n",
    "    new_reward = -0.1 + max(0, (MAX_DISTANCE - distance_to_goal(new_agent_pos, goal)) / MAX_DISTANCE) * 0.5\n",
    "\n",
    "    return new_agent_pos, distance_to_goal(new_agent_pos, goal), new_reward\n",
    "\n",
    "def target_step(pos):\n",
    "    dx, dy, dz = actions[random.randint(0, len(actions) - 1)]\n",
    "    new_pos = (pos[0] + dx, pos[1] + dy, pos[2] + dz)\n",
    "    if not is_valid(new_pos):\n",
    "        return pos\n",
    "    return new_pos\n",
    "\n",
    "# Suivi du meilleur chemin\n",
    "saved_agent_path = []\n",
    "saved_target_path = []\n",
    "shortest_length = float('inf')\n",
    "\n",
    "for episode in range(iterations):\n",
    "    agent = start_agent\n",
    "    goal = start_target\n",
    "\n",
    "    state = distances_to_goal(agent, goal)\n",
    "    total_reward = 0\n",
    "\n",
    "    path = [agent]\n",
    "    target_path = [goal]\n",
    "\n",
    "    while agent != goal:\n",
    "        if random.random() < epsilon:\n",
    "            action_idx = random.randint(0, len(actions) - 1)\n",
    "        else:\n",
    "            action_idx = np.argmax(get_q(state))\n",
    "\n",
    "        next_position, next_state, reward = agent_step(agent, action_idx, goal, path)\n",
    "        next_goal = target_step(goal)\n",
    "        total_reward += reward\n",
    "\n",
    "        old_value = get_q(state)[action_idx]\n",
    "        next_max = np.max(get_q(next_state))\n",
    "        get_q(state)[action_idx] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "\n",
    "        agent = next_position\n",
    "        goal = next_goal\n",
    "        state = next_state\n",
    "        path.append(agent)\n",
    "        target_path.append(goal)\n",
    "\n",
    "        # Sécurité pour éviter des boucles infinies\n",
    "        if len(path) > GRID_SIZE**3:\n",
    "            break\n",
    "\n",
    "    # Sauvegarde du meilleur chemin\n",
    "    if agent == goal and len(path) < shortest_length:\n",
    "        saved_agent_path = path\n",
    "        saved_target_path = target_path\n",
    "        shortest_length = len(path)\n",
    "\n",
    "    # Décroissance de l'exploration\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "\n",
    "## Plot\n",
    "display_graph(saved_agent_path, saved_target_path)\n"
   ],
   "id": "4f9c4d080b003ef9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display_animation(saved_agent_path, saved_target_path)",
   "id": "64945c194cc31928",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Chemotaxis",
   "id": "cc0227a21845d965"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Hyperparamètres\n",
    "alpha = 0.2\n",
    "gamma = 0.9\n",
    "epsilon = 0.3\n",
    "epsilon_decay = 0.99\n",
    "min_epsilon = 0.01\n",
    "iterations = 3000\n",
    "\n",
    "# Taille de la grille\n",
    "GRID_SIZE = 20\n",
    "MAX_DISTANCE = 3 * (GRID_SIZE - 1)\n",
    "\n",
    "# Départ et but\n",
    "start_agent = (0, 0, 0)\n",
    "start_target = (GRID_SIZE - 1 , GRID_SIZE - 1, GRID_SIZE - 1)\n",
    "\n",
    "# Définition des actions : (dx, dy, dz)\n",
    "actions = [\n",
    "    (1, 0, 0), (-1, 0, 0),  # droite / gauche\n",
    "    (0, 1, 0), (0, -1, 0),  # avant / arrière\n",
    "    (0, 0, 1), (0, 0, -1)   # haut / bas\n",
    "]\n",
    "\n",
    "# Initialisation Q-table\n",
    "Q = {}\n",
    "\n",
    "def distances_to_goal(pos, goal):\n",
    "    dx = pos[0] - goal[0]\n",
    "    dy = pos[1] - goal[1]\n",
    "    dz = pos[2] - goal[2]\n",
    "    return dx, dy, dz\n",
    "\n",
    "def distance_to_goal(pos, goal):\n",
    "    dx, dy, dz = distances_to_goal(pos, goal)\n",
    "    return abs(dx) + abs(dy) + abs(dz)\n",
    "\n",
    "def get_concentration(agent, goal):\n",
    "    dist = distance_to_goal(agent, goal)\n",
    "    return max(0.0, 1.0 - dist / MAX_DISTANCE)\n",
    "\n",
    "def bucket(value, step=0.1):\n",
    "    return round(np.floor(value / step) * step, 2)\n",
    "\n",
    "def get_q(state):\n",
    "    if state not in Q:\n",
    "        Q[state] = np.zeros(len(actions))\n",
    "    return Q[state]\n",
    "\n",
    "def is_valid(pos):\n",
    "    return all(0 <= p < GRID_SIZE for p in pos)\n",
    "\n",
    "def agent_step(agent, action_idx, goal, prev_concentration):\n",
    "    dx, dy, dz = actions[action_idx]\n",
    "    new_agent_pos = (agent[0] + dx, agent[1] + dy, agent[2] + dz)\n",
    "\n",
    "    if not is_valid(new_agent_pos):\n",
    "        return agent, prev_concentration, 0.0, -1\n",
    "\n",
    "    if new_agent_pos == goal:\n",
    "        return new_agent_pos, 1.0, 1.0, 10\n",
    "\n",
    "    new_concentration = get_concentration(new_agent_pos, goal)\n",
    "    gradient = new_concentration - prev_concentration\n",
    "\n",
    "    reward = gradient * 5.0 - 0.05\n",
    "\n",
    "    return new_agent_pos, new_concentration, gradient, reward\n",
    "\n",
    "def target_step(pos):\n",
    "    dx, dy, dz = actions[random.randint(0, len(actions) - 1)]\n",
    "    new_pos = (pos[0] + dx, pos[1] + dy, pos[2] + dz)\n",
    "    if not is_valid(new_pos):\n",
    "        return pos\n",
    "    return new_pos\n",
    "\n",
    "# Suivi du meilleur chemin\n",
    "saved_agent_path = []\n",
    "saved_target_path = []\n",
    "shortest_length = float('inf')\n",
    "\n",
    "for episode in range(iterations):\n",
    "    agent = start_agent\n",
    "    goal = start_target\n",
    "\n",
    "    current_concentration = get_concentration(agent, goal)\n",
    "    previous_concentration = current_concentration  # au début, même valeur\n",
    "    state = (bucket(current_concentration), bucket(0.0))\n",
    "    total_reward = 0\n",
    "\n",
    "    path = [agent]\n",
    "    target_path = [goal]\n",
    "\n",
    "    # Pendant l'exploration :\n",
    "    while agent != goal:\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            action_idx = random.randint(0, len(actions) - 1)\n",
    "        else:\n",
    "            action_idx = np.argmax(get_q(state))\n",
    "\n",
    "        next_position, new_concentration, gradient, reward = agent_step(agent, action_idx, goal, previous_concentration)\n",
    "        next_state = (bucket(new_concentration), bucket(gradient))\n",
    "\n",
    "        # epsilon = np.clip(epsilon * (0.99 if gradient > 0 else 1.01), min_epsilon, 1.0)\n",
    "\n",
    "        agent = next_position\n",
    "        goal = target_step(goal)\n",
    "\n",
    "        old_value = get_q(state)[action_idx]\n",
    "        next_max = np.max(get_q(next_state))\n",
    "        get_q(state)[action_idx] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "\n",
    "        state = next_state\n",
    "        previous_concentration = current_concentration\n",
    "        current_concentration = new_concentration\n",
    "\n",
    "        path.append(agent)\n",
    "        target_path.append(goal)\n",
    "\n",
    "        if len(path) > GRID_SIZE**3:\n",
    "            break\n",
    "\n",
    "\n",
    "    # Sauvegarde du meilleur chemin\n",
    "    if agent == goal and len(path) < shortest_length:\n",
    "        saved_agent_path = path\n",
    "        saved_target_path = target_path\n",
    "        shortest_length = len(path)\n",
    "\n",
    "    # Décroissance de l'exploration\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "\n",
    "## Plot\n",
    "display_graph(saved_agent_path, saved_target_path)\n"
   ],
   "id": "21876d7ce7d5af17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display_animation(saved_agent_path, saved_target_path)",
   "id": "aa98ca87f27bbc52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2D Chemiotaxis",
   "id": "1c7d0bcfe9c5d39c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Hyperparamètres\n",
    "alpha = 0.2\n",
    "gamma = 0.9\n",
    "epsilon = 0.3\n",
    "epsilon_decay = 0.99\n",
    "min_epsilon = 0.01\n",
    "iterations = 500\n",
    "\n",
    "# Taille de la grille\n",
    "GRID_SIZE = 60\n",
    "MAX_DISTANCE = 2 * (GRID_SIZE - 1)\n",
    "\n",
    "# Départ et but\n",
    "start_agent = (10, 10)\n",
    "start_target = (50 , 50)\n",
    "\n",
    "# Définition des actions : (dx, dy)\n",
    "actions = [\n",
    "    (1, 0,), (-1, 0),  # droite / gauche\n",
    "    (0, 1), (0, -1),  # avant / arrière\n",
    "]\n",
    "\n",
    "# Initialisation Q-table\n",
    "Q = {}\n",
    "\n",
    "def distances_to_goal(pos, goal):\n",
    "    dx = pos[0] - goal[0]\n",
    "    dy = pos[1] - goal[1]\n",
    "    return dx, dy\n",
    "\n",
    "def distance_to_goal(pos, goal):\n",
    "    dx, dy = distances_to_goal(pos, goal)\n",
    "    return abs(dx) + abs(dy)\n",
    "\n",
    "def get_concentration(agent, goal):\n",
    "    dist = distance_to_goal(agent, goal)\n",
    "    return max(0.0, 1.0 - dist / MAX_DISTANCE)\n",
    "\n",
    "def bucket(value, step=0.1):\n",
    "    return round(np.floor(value / step) * step, 2)\n",
    "\n",
    "def get_q(state):\n",
    "    if state not in Q:\n",
    "        Q[state] = np.zeros(len(actions))\n",
    "    return Q[state]\n",
    "\n",
    "def is_valid(pos):\n",
    "    return all(0 <= p < GRID_SIZE for p in pos)\n",
    "\n",
    "def agent_step(agent, action_idx, goal, prev_concentration):\n",
    "    dx, dy = actions[action_idx]\n",
    "    new_agent_pos = (agent[0] + dx, agent[1] + dy)\n",
    "\n",
    "    if not is_valid(new_agent_pos):\n",
    "        return agent, prev_concentration, 0.0, -1\n",
    "\n",
    "    if new_agent_pos == goal:\n",
    "        return new_agent_pos, 1.0, 1.0, 10\n",
    "\n",
    "    new_concentration = get_concentration(new_agent_pos, goal)\n",
    "    gradient = new_concentration - prev_concentration\n",
    "\n",
    "    reward = gradient * 5.0 - 0.05\n",
    "\n",
    "    return new_agent_pos, new_concentration, gradient, reward\n",
    "\n",
    "def target_step(pos):\n",
    "    dx, dy = actions[random.randint(0, len(actions) - 1)]\n",
    "    new_pos = (pos[0] + dx, pos[1] + dy)\n",
    "    if not is_valid(new_pos):\n",
    "        return pos\n",
    "    return new_pos\n",
    "\n",
    "# Suivi du meilleur chemin\n",
    "saved_agent_path = []\n",
    "saved_target_path = []\n",
    "shortest_length = float('inf')\n",
    "\n",
    "for episode in range(iterations):\n",
    "    agent = start_agent\n",
    "    goal = start_target\n",
    "\n",
    "    current_concentration = get_concentration(agent, goal)\n",
    "    previous_concentration = current_concentration  # au début, même valeur\n",
    "    state = (bucket(current_concentration), bucket(0.0))\n",
    "    total_reward = 0\n",
    "\n",
    "    path = [agent]\n",
    "    target_path = [goal]\n",
    "\n",
    "    # Pendant l'exploration :\n",
    "    while agent != goal:\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            action_idx = random.randint(0, len(actions) - 1)\n",
    "        else:\n",
    "            action_idx = np.argmax(get_q(state))\n",
    "\n",
    "        next_position, new_concentration, gradient, reward = agent_step(agent, action_idx, goal, previous_concentration)\n",
    "        next_state = (bucket(new_concentration), bucket(gradient))\n",
    "\n",
    "        # epsilon = np.clip(epsilon * (0.99 if gradient > 0 else 1.01), min_epsilon, 1.0)\n",
    "\n",
    "        agent = next_position\n",
    "        goal = target_step(goal)\n",
    "\n",
    "        old_value = get_q(state)[action_idx]\n",
    "        next_max = np.max(get_q(next_state))\n",
    "        get_q(state)[action_idx] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "\n",
    "        state = next_state\n",
    "        previous_concentration = current_concentration\n",
    "        current_concentration = new_concentration\n",
    "\n",
    "        path.append(agent)\n",
    "        target_path.append(goal)\n",
    "\n",
    "        if len(path) > GRID_SIZE**3:\n",
    "            break\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Epoch : {episode}\")\n",
    "\n",
    "    # Sauvegarde du meilleur chemin\n",
    "    if agent == goal and len(path) < shortest_length:\n",
    "        saved_agent_path = path\n",
    "        saved_target_path = target_path\n",
    "        shortest_length = len(path)\n",
    "        print(f\"New shortest path length: {shortest_length} on episode: {episode}\")\n",
    "\n",
    "    # Décroissance de l'exploration\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "\n",
    "\n",
    "## Plot\n",
    "display_graph_2d(saved_agent_path, saved_target_path, GRID_SIZE)"
   ],
   "id": "81779b879e99bd6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display_animation_2d(saved_agent_path, saved_target_path)",
   "id": "57954b46f63abcde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fa7d4c3c90835c3a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
